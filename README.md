# ğŸ•µï¸â€â™€ï¸ fraud-detector

Detecting fraudulent auto insurance claims using machine learning (Logistic Regression, XGBoost) with a strong focus on **model performance, interpretability, and pipeline modularity**.


**Goal:** Accurately flag potentially fraudulent claims to help insurers reduce financial losses.

---

## ğŸ“Œ Project Overview

This project builds a machine learning pipeline to detect fraud in structured auto insurance claim data. It includes:  
- Cleaning & preprocessing raw input data  
- Handling inliers vs. outliers separately  
- Training and comparing multiple models  
- Generating predictions for new data  
- Laying the groundwork for future explainability  

---

## ğŸ’¡ Why This Matters  
Insurance fraud is a high-impact use case for ML. This project shows how:  
- Domain-specific preprocessing (inliers/outliers) boosts performance  
- Explainability tools like SHAP make black-box models more trustworthy  
- Modular pipelines keep experiments clean and reproducible  

---

## ğŸ§  Techniques Used

- Supervised Learning: Logistic Regression, XGBoost  
- Outlier Handling: IQR-based removal  
- Feature Engineering: One-hot encoding (LogReg), Ordinal encoding (XGBoost)  
- Feature Scaling: StandardScaler (for Logistic Regression)
- Evaluation Metrics: Accuracy, Precision, Recall, F1, ROC AUC
- Class Imbalance Handling: scale_pos_weight in XGBoost   
- Model Explainability: SHAP (SHapley Additive Explanations)  

---

## ğŸ“ Project Structure
```
fraud-detector/
â”‚
â”œâ”€â”€ data/                   # Input data and prediction results
â”œâ”€â”€ models/                 # Saved models and preprocessors (.pkl)
â”œâ”€â”€ notebooks/              # EDA, modeling, and inference notebooks
â”œâ”€â”€ src/                    # Modular Python code
â”‚   â”œâ”€â”€ preprocessing.py        # Data cleaning, outlier removal
â”‚   â”œâ”€â”€ prep_features.py        # Feature encoding and scaling
â”‚   â”œâ”€â”€ train.py                # Model training and evaluation
â”‚   â”œâ”€â”€ inference.py            # Batch inference on new data
â”‚   â”œâ”€â”€ visualization.py        # SHAP explanations and feature name utilities
â”‚   â””â”€â”€ feature_selection.py    # Feature filtering (optional experiment)
â””â”€â”€ README.md               # Project documentation
```
---

## ğŸ”§ Function Overview

The pipeline is built from reusable, modular functions:  

- ğŸ§¼ `clean_data()` & `remove_outliers_iqr()`  
  â†’ Clean the dataset, fix dates, and isolate outliers  

- ğŸ“‰ `check_class_ratio()`  
  â†’ Explore fraud distribution across inliers and outliers   

- âš™ï¸ `prepare_for_logreg()` & `prepare_for_xgb()`  
  â†’ Preprocess features for each model type (scaling + encoding)

- ğŸ·ï¸ `get_feature_names_from_column_transformer()`
  â†’ Extract readable feature names from fitted ColumnTransformer  

- ğŸ§  `train_models()`  
  â†’ Train four models (inliers/outliers Ã— LR/XGB) and save them  

- ğŸ“ˆ `build_summary_table()`  
  â†’ Tabulate model performance across key metrics  

- ğŸ” `predict_new_data()`  
  â†’ Run saved models on new incoming claims for fraud prediction

- ğŸ§  `explain_model_with_shap()`  
  â†’ Visualize top drivers of XGBoost model predictions using SHAP    

---

### ğŸ“’ Notebooks

- `01_data_exploration.ipynb`: Clean raw data, explore distributions, and split into inliers/outliers  
- `02_modeling.ipynb`: Train and compare 4 models (LR & XGB on inliers/outliers)  
- `03_inference.ipynb`: Apply trained models to new test claims and generate predictions    

---

## ğŸš€ Current Goals

- Build interpretable and modular fraud detection models  
- Compare performance across inliers and outliers  
- Optimize for recall and precision to balance false positives  
- Prepare for downstream tasks like SHAP-based interpretation  

---

### ğŸ“¤ Example Output

After running inference on new claims, you get an output CSV like:  

| policy_number | incident_date | ... | total_claim_amount | fraud_predicted |
|---------------|----------------|-----|---------------------|------------------|
| 54321         | 2023-01-14     | ... | 8200.0              | 1                |
| 67890         | 2023-02-11     | ... | 3100.0              | 0                |

---

## ğŸ”® Future Improvements

- Add model-based alert thresholds for production deployment  
- Test hybrid models using both structured and textual features  
- Deploy via Streamlit or Gradio for user-friendly interaction  
- Package the pipeline into an API for production integration
- Automate model monitoring and threshold adjustment  
